{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "2a8e9289390a0339"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import packages",
   "id": "7ccfef15ecb25596"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T16:19:20.600004Z",
     "start_time": "2024-09-18T16:19:06.760314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.lite.python.schema_py_generated import Model\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from timm.data import create_dataset\n"
   ],
   "id": "ae3751a9d2721eda",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T17:39:13.249661Z",
     "start_time": "2024-09-18T17:39:13.232908Z"
    }
   },
   "cell_type": "code",
   "source": "os.getcwd()",
   "id": "d9226f4475466134",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\marti\\\\Desktop\\\\Škola\\\\Diplomova prace\\\\pytorch-image-models'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inspecting timm",
   "id": "41e9fff1c9e91a8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T19:44:39.185288Z",
     "start_time": "2024-09-10T19:44:39.153444Z"
    }
   },
   "cell_type": "code",
   "source": "len(timm.list_models())",
   "id": "3a6dfc46a0442e63",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1133"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T19:44:42.216966Z",
     "start_time": "2024-09-10T19:44:42.187339Z"
    }
   },
   "cell_type": "code",
   "source": "len(timm.list_models(pretrained=True))",
   "id": "11c6348359559301",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1459"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load data",
   "id": "dc55c957c466e11d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-10T19:37:49.932475Z",
     "start_time": "2024-09-10T19:37:49.869706Z"
    }
   },
   "source": [
    "# Construct a tf.data.Dataset\n",
    "ds = tfds.load('mnist', split='train', as_supervised=True, shuffle_files=True)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T20:57:01.362251Z",
     "start_time": "2024-09-10T20:57:01.331173Z"
    }
   },
   "cell_type": "code",
   "source": "os.chdir(r'C:\\\\Users\\\\marti\\\\Desktop\\\\Škola\\\\Diplomova prace\\\\Imagenette\\\\Imagenette2')",
   "id": "57dee7948464f2c2",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T20:59:57.848907Z",
     "start_time": "2024-09-10T20:57:07.839837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "noise_level = '5' # '5' or '50'\n",
    "outdir = 'noisy' + noise_level\n",
    "df = pd.read_csv('noisy_imagenette.csv', sep=',')\n",
    "for inpath, label, is_val in zip(df['path'], df['noisy_labels_' + noise_level], df['is_valid']):\n",
    "    if is_val:\n",
    "        subdir = 'val'\n",
    "    else:\n",
    "        subdir = 'train'\n",
    "    outpath = os.path.join(outdir, subdir, label)\n",
    "    os.makedirs(outpath, exist_ok=True)\n",
    "    shutil.copy2(inpath, outpath)"
   ],
   "id": "66e7cd190285630",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T15:32:57.331186Z",
     "start_time": "2024-09-11T15:32:54.826989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mnist = load_dataset(\"ylecun/mnist\")"
   ],
   "id": "b668bf2fbad7409b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ],
   "id": "30aef06a2251a87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "os.chdir(r'C:\\\\Users\\\\marti\\\\Desktop\\\\Škola\\\\Diplomova prace\\\\CIFAR')",
   "id": "1017554c62b0b04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T15:33:57.336099Z",
     "start_time": "2024-09-11T15:33:30.593796Z"
    }
   },
   "cell_type": "code",
   "source": "cifar = create_dataset('torch/cifar10', 'cifar10', download=True, split='train')",
   "id": "680632331ad0ab12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar10\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:23<00:00, 7390545.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cifar10\\cifar-10-python.tar.gz to cifar10\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "d1f5412916abb448"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T17:37:41.961643Z",
     "start_time": "2024-09-18T17:37:41.936471Z"
    }
   },
   "cell_type": "code",
   "source": "timm.list_models('efficientnet*', pretrained=True)",
   "id": "67955d8d0d80c9f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['efficientnet_b0.ra4_e3600_r224_in1k',\n",
       " 'efficientnet_b0.ra_in1k',\n",
       " 'efficientnet_b1.ft_in1k',\n",
       " 'efficientnet_b1.ra4_e3600_r240_in1k',\n",
       " 'efficientnet_b1_pruned.in1k',\n",
       " 'efficientnet_b2.ra_in1k',\n",
       " 'efficientnet_b2_pruned.in1k',\n",
       " 'efficientnet_b3.ra2_in1k',\n",
       " 'efficientnet_b3_pruned.in1k',\n",
       " 'efficientnet_b4.ra2_in1k',\n",
       " 'efficientnet_b5.sw_in12k',\n",
       " 'efficientnet_b5.sw_in12k_ft_in1k',\n",
       " 'efficientnet_el.ra_in1k',\n",
       " 'efficientnet_el_pruned.in1k',\n",
       " 'efficientnet_em.ra2_in1k',\n",
       " 'efficientnet_es.ra_in1k',\n",
       " 'efficientnet_es_pruned.in1k',\n",
       " 'efficientnet_lite0.ra_in1k',\n",
       " 'efficientnetv2_rw_m.agc_in1k',\n",
       " 'efficientnetv2_rw_s.ra2_in1k',\n",
       " 'efficientnetv2_rw_t.ra2_in1k']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T15:27:18.200543Z",
     "start_time": "2024-09-11T15:27:14.144196Z"
    }
   },
   "cell_type": "code",
   "source": "model = timm.create_model('resnet50d', pretrained=True, num_classes=10)",
   "id": "aca7236480d43755",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/103M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9afe983563b84aa1b077d86e230a984f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\AppData\\Roaming\\Python\\Python38\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\marti\\.cache\\huggingface\\hub\\models--timm--resnet50d.ra2_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T15:28:09.077045Z",
     "start_time": "2024-09-11T15:28:09.062770Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "1b2dfd810818921a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T16:19:33.991578Z",
     "start_time": "2024-09-18T16:19:33.976406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%writefile train.py\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import timm\n",
    "import timm.data\n",
    "import timm.loss\n",
    "import timm.optim\n",
    "import timm.utils\n",
    "import torch\n",
    "import torchmetrics\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "\n",
    "from pytorch_accelerated.callbacks import SaveBestModelCallback\n",
    "from pytorch_accelerated.trainer import Trainer, DEFAULT_CALLBACKS\n",
    "\n",
    "\n",
    "def create_datasets(image_size, data_mean, data_std, train_path, val_path):\n",
    "    train_transforms = timm.data.create_transform(\n",
    "        input_size=image_size,\n",
    "        is_training=True,\n",
    "        mean=data_mean,\n",
    "        std=data_std,\n",
    "        auto_augment=\"rand-m7-mstd0.5-inc1\",\n",
    "    )\n",
    "\n",
    "    eval_transforms = timm.data.create_transform(\n",
    "        input_size=image_size, mean=data_mean, std=data_std\n",
    "    )\n",
    "\n",
    "    train_dataset = timm.data.dataset.ImageDataset(\n",
    "        train_path, transform=train_transforms\n",
    "    )\n",
    "    eval_dataset = timm.data.dataset.ImageDataset(val_path, transform=eval_transforms)\n",
    "\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "\n",
    "class TimmMixupTrainer(Trainer):\n",
    "    def __init__(self, eval_loss_fn, mixup_args, num_classes, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.eval_loss_fn = eval_loss_fn\n",
    "        self.num_updates = None\n",
    "        self.mixup_fn = timm.data.Mixup(**mixup_args)\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(num_classes=num_classes)\n",
    "        self.ema_accuracy = torchmetrics.Accuracy(num_classes=num_classes)\n",
    "        self.ema_model = None\n",
    "\n",
    "    def create_scheduler(self):\n",
    "        return timm.scheduler.CosineLRScheduler(\n",
    "            self.optimizer,\n",
    "            t_initial=self.run_config.num_epochs,\n",
    "            cycle_decay=0.5,\n",
    "            lr_min=1e-6,\n",
    "            t_in_epochs=True,\n",
    "            warmup_t=3,\n",
    "            warmup_lr_init=1e-4,\n",
    "            cycle_limit=1,\n",
    "        )\n",
    "\n",
    "    def training_run_start(self):\n",
    "        # Model EMA requires the model without a DDP wrapper and before sync batchnorm conversion\n",
    "        self.ema_model = timm.utils.ModelEmaV2(\n",
    "            self._accelerator.unwrap_model(self.model), decay=0.9\n",
    "        )\n",
    "        if self.run_config.is_distributed:\n",
    "            self.model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(self.model)\n",
    "\n",
    "    def train_epoch_start(self):\n",
    "        super().train_epoch_start()\n",
    "        self.num_updates = self.run_history.current_epoch * len(self._train_dataloader)\n",
    "\n",
    "    def calculate_train_batch_loss(self, batch):\n",
    "        xb, yb = batch\n",
    "        mixup_xb, mixup_yb = self.mixup_fn(xb, yb)\n",
    "        return super().calculate_train_batch_loss((mixup_xb, mixup_yb))\n",
    "\n",
    "    def train_epoch_end(\n",
    "        self,\n",
    "    ):\n",
    "        self.ema_model.update(self.model)\n",
    "        self.ema_model.eval()\n",
    "\n",
    "        if hasattr(self.optimizer, \"sync_lookahead\"):\n",
    "            self.optimizer.sync_lookahead()\n",
    "\n",
    "    def scheduler_step(self):\n",
    "        self.num_updates += 1\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step_update(num_updates=self.num_updates)\n",
    "\n",
    "    def calculate_eval_batch_loss(self, batch):\n",
    "        with torch.no_grad():\n",
    "            xb, yb = batch\n",
    "            outputs = self.model(xb)\n",
    "            val_loss = self.eval_loss_fn(outputs, yb)\n",
    "            self.accuracy.update(outputs.argmax(-1), yb)\n",
    "\n",
    "            ema_model_preds = self.ema_model.module(xb).argmax(-1)\n",
    "            self.ema_accuracy.update(ema_model_preds, yb)\n",
    "\n",
    "        return {\"loss\": val_loss, \"model_outputs\": outputs, \"batch_size\": xb.size(0)}\n",
    "\n",
    "    def eval_epoch_end(self):\n",
    "        super().eval_epoch_end()\n",
    "\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step(self.run_history.current_epoch + 1)\n",
    "\n",
    "        self.run_history.update_metric(\"accuracy\", self.accuracy.compute().cpu())\n",
    "        self.run_history.update_metric(\n",
    "            \"ema_model_accuracy\", self.ema_accuracy.compute().cpu()\n",
    "        )\n",
    "        self.accuracy.reset()\n",
    "        self.ema_accuracy.reset()\n",
    "\n",
    "\n",
    "def main(data_path):\n",
    "\n",
    "    # Set training arguments, hardcoded here for clarity\n",
    "    image_size = (224, 224)\n",
    "    lr = 5e-3\n",
    "    smoothing = 0.1\n",
    "    mixup = 0.2\n",
    "    cutmix = 1.0\n",
    "    batch_size = 32\n",
    "    bce_target_thresh = 0.2\n",
    "    num_epochs = 40\n",
    "\n",
    "    data_path = Path(data_path)\n",
    "    train_path = data_path / \"train\"\n",
    "    val_path = data_path / \"val\"\n",
    "    num_classes = len(list(train_path.iterdir()))\n",
    "\n",
    "    mixup_args = dict(\n",
    "        mixup_alpha=mixup,\n",
    "        cutmix_alpha=cutmix,\n",
    "        label_smoothing=smoothing,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "\n",
    "    # Create model using timm\n",
    "    model = timm.create_model(\n",
    "        \"resnet50d\", pretrained=False, num_classes=num_classes, drop_path_rate=0.05\n",
    "    )\n",
    "\n",
    "    # Load data config associated with the model to use in data augmentation pipeline\n",
    "    data_config = timm.data.resolve_data_config({}, model=model, verbose=True)\n",
    "    data_mean = data_config[\"mean\"]\n",
    "    data_std = data_config[\"std\"]\n",
    "\n",
    "    # Create training and validation datasets\n",
    "    train_dataset, eval_dataset = create_datasets(\n",
    "        train_path=train_path,\n",
    "        val_path=val_path,\n",
    "        image_size=image_size,\n",
    "        data_mean=data_mean,\n",
    "        data_std=data_std,\n",
    "    )\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = timm.optim.create_optimizer_v2(\n",
    "        model, opt=\"lookahead_AdamW\", lr=lr, weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    # As we are using Mixup, we can use BCE during training and CE for evaluation\n",
    "    train_loss_fn = timm.loss.BinaryCrossEntropy(\n",
    "        target_threshold=bce_target_thresh, smoothing=smoothing\n",
    "    )\n",
    "    validate_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Create trainer and start training\n",
    "    trainer = TimmMixupTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_func=train_loss_fn,\n",
    "        eval_loss_fn=validate_loss_fn,\n",
    "        mixup_args=mixup_args,\n",
    "        num_classes=num_classes,\n",
    "        callbacks=[\n",
    "            *DEFAULT_CALLBACKS,\n",
    "            SaveBestModelCallback(watch_metric=\"accuracy\", greater_is_better=True),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainer.train(\n",
    "        per_device_batch_size=batch_size,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        num_epochs=num_epochs,\n",
    "        create_scheduler_fn=trainer.create_scheduler,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of training script using timm.\")\n",
    "    parser.add_argument(\"--data_dir\", required=True, help=\"The data folder on disk.\")\n",
    "    args = parser.parse_args()\n",
    "    main(args.data_dir)"
   ],
   "id": "a1ede8213aa0fc4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T16:19:38.053650Z",
     "start_time": "2024-09-18T16:19:38.000563Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.is_available())",
   "id": "78779de67eafb5b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T19:13:14.773762Z",
     "start_time": "2024-09-11T19:13:14.758138Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.__version__)",
   "id": "cc39c6e802f07b5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cpu\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T19:22:54.338408Z",
     "start_time": "2024-09-11T19:22:54.307150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.path.exists('C:\\\\Users\\\\marti\\\\Desktop\\\\Škola\\\\Diplomova prace\\\\Imagenette\\imagenette2'))"
   ],
   "id": "cdf50e8f3db0b344",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T22:34:44.728403Z",
     "start_time": "2024-09-18T22:34:44.697112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "import random"
   ],
   "id": "2b7ad3111308737a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T22:20:36.772704Z",
     "start_time": "2024-09-18T22:20:36.757077Z"
    }
   },
   "cell_type": "code",
   "source": "wandb.require(\"core\")",
   "id": "ecb1a60d32373108",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T22:23:28.677891Z",
     "start_time": "2024-09-18T22:20:44.412925Z"
    }
   },
   "cell_type": "code",
   "source": "wandb.login()",
   "id": "f794080ec6502258",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "wandb: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\marti\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T22:34:56.056805Z",
     "start_time": "2024-09-18T22:34:47.559369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "\n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ],
   "id": "9b166ad0ca851a4f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing last run (ID:yttx80xy) before initializing another..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85e2a8e8ff934789abe3968b8b1d78b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">skilled-pond-1</strong> at: <a href='https://wandb.ai/martinhruska_2000/my-awesome-project/runs/yttx80xy' target=\"_blank\">https://wandb.ai/martinhruska_2000/my-awesome-project/runs/yttx80xy</a><br/> View project at: <a href='https://wandb.ai/martinhruska_2000/my-awesome-project' target=\"_blank\">https://wandb.ai/martinhruska_2000/my-awesome-project</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240919_003427-yttx80xy\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Successfully finished last run (ID:yttx80xy). Initializing new run:<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\marti\\Desktop\\Škola\\Diplomova prace\\pytorch-image-models\\wandb\\run-20240919_003447-l6twxo1b</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/martinhruska_2000/my-awesome-project/runs/l6twxo1b' target=\"_blank\">glowing-night-2</a></strong> to <a href='https://wandb.ai/martinhruska_2000/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/martinhruska_2000/my-awesome-project' target=\"_blank\">https://wandb.ai/martinhruska_2000/my-awesome-project</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/martinhruska_2000/my-awesome-project/runs/l6twxo1b' target=\"_blank\">https://wandb.ai/martinhruska_2000/my-awesome-project/runs/l6twxo1b</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e22a3f52781473692fc6a755f204a69"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▃▃▇▆██▇</td></tr><tr><td>loss</td><td>█▅▄▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.76474</td></tr><tr><td>loss</td><td>0.26277</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glowing-night-2</strong> at: <a href='https://wandb.ai/martinhruska_2000/my-awesome-project/runs/l6twxo1b' target=\"_blank\">https://wandb.ai/martinhruska_2000/my-awesome-project/runs/l6twxo1b</a><br/> View project at: <a href='https://wandb.ai/martinhruska_2000/my-awesome-project' target=\"_blank\">https://wandb.ai/martinhruska_2000/my-awesome-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240919_003447-l6twxo1b\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T16:36:36.539787Z",
     "start_time": "2024-09-23T16:36:34.103298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "8b1dfba57f43c546",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T16:36:40.883629Z",
     "start_time": "2024-09-23T16:36:40.121021Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.memory_summary())",
   "id": "58aa4f8c05cf398b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T16:42:52.621881Z",
     "start_time": "2024-09-23T16:42:52.590629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ],
   "id": "94835cff7035986b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T16:48:21.766012Z",
     "start_time": "2024-09-23T16:48:20.042273Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.list_gpu_processes())",
   "id": "bdcc07952cb1362b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:0\n",
      "no processes are running\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T16:48:55.597946Z",
     "start_time": "2024-09-23T16:48:53.824739Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.mem_get_info())",
   "id": "8e60733d98e9ddf0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3458675508, 4294639616)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T16:49:44.858748Z",
     "start_time": "2024-09-23T16:49:44.827044Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.memory_stats())",
   "id": "61f6f252d6f1b5d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_device_alloc', 0), ('num_device_free', 0), ('num_ooms', 0), ('num_sync_all_streams', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 0), ('requested_bytes.all.current', 0), ('requested_bytes.all.freed', 0), ('requested_bytes.all.peak', 0), ('requested_bytes.large_pool.allocated', 0), ('requested_bytes.large_pool.current', 0), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 0), ('requested_bytes.small_pool.allocated', 0), ('requested_bytes.small_pool.current', 0), ('requested_bytes.small_pool.freed', 0), ('requested_bytes.small_pool.peak', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T16:50:21.850062Z",
     "start_time": "2024-09-23T16:50:21.818841Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.memory_snapshot())",
   "id": "b7e03d0ea464ef2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T16:50:34.508853Z",
     "start_time": "2024-09-23T16:50:34.493340Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.memory_allocated())",
   "id": "286a65ab4199bcac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T16:51:21.824518Z",
     "start_time": "2024-09-23T16:51:21.793126Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.max_memory_allocated())",
   "id": "d80d1cfc90ca3a0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T16:51:01.281435Z",
     "start_time": "2024-09-23T16:51:01.249878Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.cuda.memory_reserved())",
   "id": "39a66697866bbbcd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T16:53:12.777650Z",
     "start_time": "2024-09-23T16:53:12.762021Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.reset_peak_memory_stats()",
   "id": "a5a84fd299fea4de",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "805d31422bd4c256"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
